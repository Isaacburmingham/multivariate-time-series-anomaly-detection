{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b72cb330",
   "metadata": {},
   "source": [
    "## Isaac Burmingham\n",
    "### Algorithm Modeling Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfafc057",
   "metadata": {},
   "source": [
    "This juypter notebook writes 3 .py files containing custom functions to be used in the exploratory and anomaly detection notebooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bc4498ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting algorithm_modeling.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile algorithm_modeling.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import reduce\n",
    "import os\n",
    "import re\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM, Dropout\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "def LSTM_model(units,train_generator,test_generator,dactivation='linear', \\\n",
    "               n_outputs=1,epochs=100,learning_rate=0.001,dropout=0.1,alpha=0.05):\n",
    "    \n",
    "    trainX,trainY = train_generator[0]\n",
    "    testX,testY = test_generator[0]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units,input_shape=(trainX.shape[1],trainX.shape[2]),activation='tanh',#return_sequences=True,\n",
    "                  recurrent_activation='sigmoid',recurrent_dropout=0,unroll=False,use_bias=True))\n",
    "   # model.add(LSTM(units,activation='tanh',\n",
    "    #          recurrent_activation='sigmoid',recurrent_dropout=0,unroll=False,use_bias=True))\n",
    "    #model.add(LeakyReLU(alpha=alpha))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(n_outputs,activation=dactivation))\n",
    "    \n",
    "    callback = EarlyStopping(monitor='loss',patience = 10, mode='auto')\n",
    "    \n",
    "    adam = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(loss='mse',optimizer=adam, metrics=['mae','mse'])\n",
    "    \n",
    "    #with tf.device('XLA_GPU'):\n",
    "        #print(tf.device('/gpu:0'))\n",
    "    #    print(tf.test.is_built_with_cuda())\n",
    "        \n",
    "    hist = model.fit(train_generator,epochs=epochs, validation_data=test_generator,callbacks=callback,\\\n",
    "                     verbose=2,shuffle=False,batch_size=10)\n",
    "    \n",
    "    return model,hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1a2960e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting custom_functions.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom_functions.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#Below function is pulled from Sada Narayanappa's Github with few adjustments. \n",
    "# Purpose is to ensure to features are highly correlated with Time\n",
    "\n",
    "#See citation file for details \n",
    "\n",
    "# Find any sensor highly correlated with time and drop them.\n",
    "def detectTimeCorrelated(df, timecol=\"time\", val=0.94, **kwargs):\n",
    "    timecol = df.columns[0]\n",
    "    \n",
    "    timeser = pd.Series(df[[timecol]].values.reshape(-1))\n",
    "    if ( timeser.dtype != np.number ):\n",
    "        timeser = pd.Series(pd.to_datetime(timeser).values.astype(int))\n",
    "    \n",
    "    \n",
    "    DROP_INDEX = 0; # Debugging\n",
    "    corcols    = []\n",
    "    for sensor in df.columns:\n",
    "        if (sensor == timecol ):\n",
    "            continue;\n",
    "        #print(f\"#Testing {sensor}...\")\n",
    "        # The following code tries to detect correlation by dropping first 8 or last 8 values\n",
    "        # sometimes dropping first few will show correlation due to start up times\n",
    "        sensorSeries = pd.Series(df[sensor].values.reshape(-1))\n",
    "        for i in range(8):\n",
    "            c1 = timeser[i:].corr(sensorSeries[i:])\n",
    "            c2 = timeser[i:].corr(sensorSeries[:-i])\n",
    "            if np.abs(c1) >= val or np.abs(c2) >= val:\n",
    "                corcols.append(sensor)\n",
    "                DROP_INDEX = max(DROP_INDEX, i) #lets drop first few rows\n",
    "                break;\n",
    "                \n",
    "    #print(f\"#Time Cor: #{len(timeCorSensors)}, #Shape before:{df.shape}\")\n",
    "    #df.drop(timeCorSensors, axis=1, inplace=True)\n",
    "    #df = df[DROP_INDEX:]\n",
    "    #print(f\"#After dropping: {DROP_INDEX} =>{df.shape}\")\n",
    "        \n",
    "    return corcols\n",
    "\n",
    "def reshape_predictions(label,yhat_train,yhat_test,timesteps):\n",
    "    yhat_train_p =  np.empty(shape=[label.shape[0],])\n",
    "    yhat_train_p[:] = np.nan\n",
    "    yhat_train.shape = yhat_train.shape[0]\n",
    "    yhat_train_p.shape = yhat_train_p.shape[0]\n",
    "    yhat_train_p[int(timesteps):len(yhat_train)+int(timesteps)] = yhat_train\n",
    "    \n",
    "    yhat_test_p =  np.empty(shape=[label.shape[0],])\n",
    "    yhat_test_p[:] = np.nan\n",
    "    yhat_test.shape = yhat_test.shape[0]\n",
    "    yhat_test_p.shape = yhat_test_p.shape[0]\n",
    "    yhat_test_p[len(yhat_train)+(int(timesteps)*2):len(label)] = yhat_test\n",
    "    \n",
    "    return yhat_train_p, yhat_test_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a1dcf27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting nonparametric_dynamic_thresholding.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile nonparametric_dynamic_thresholding.py\n",
    "\n",
    "# The below functions were taken from NASA JPL's 'telenanom' project that proposed the idea of nonparametric dynamic \n",
    "# thresholding. \n",
    "# The functions have been slightly modified for this project. I've made a few more parameters adjustable like p from\n",
    "# the detect_anomalies function for better testing\n",
    "\n",
    "# Standard modules\n",
    "import progressbar\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import more_itertools as mit\n",
    "from elasticsearch import Elasticsearch\n",
    "import time\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "from scipy.stats import norm\n",
    "\n",
    "original_author = 'Peter Schneider'\n",
    "mod_by = 'Isaac Burmingham'\n",
    "\n",
    "\n",
    "# number of values to evaluate in each batch\n",
    "batch_size = 70\n",
    "# number of trailing batches to use in error calculation\n",
    "window_size = 30\n",
    "# determines window size used in EWMA smoothing (percentage of total values for channel)\n",
    "smoothing_perc = 0.05\n",
    "# num previous timesteps provided to model to predict future values\n",
    "l_s = 250\n",
    "# number of values surrounding an error that are brought into the sequence (promotes grouping on nearby sequences\n",
    "error_buffer = 100\n",
    "# minimum percent decrease between max errors in anomalous sequences (used for pruning)\n",
    "p = 0.35\n",
    "\n",
    "\n",
    "def get_errors(y_test, y_hat, batch_size=70, window_size=30,smoothing_perc=0.05, anom=None, smoothed=True):\n",
    "    \"\"\"Calculate the difference between predicted telemetry values and actual values, then smooth residuals using\n",
    "    ewma to encourage identification of sustained errors/anomalies.\n",
    "\n",
    "    Inputs:\n",
    "        y_test (np array): array of test targets corresponding to true values to be predicted at end of each sequence\n",
    "        y_hat (np array): predicted test values for each timestep in y_test\n",
    "        anom (dict): contains anomaly information for a given input stream\n",
    "        smoothed (bool): If False, return unsmooothed errors (used for assessing quality of predictions)\n",
    "\n",
    "\n",
    "    Outputs:\n",
    "        e (list): unsmoothed errors (residuals)\n",
    "        e_s (list): smoothed errors (residuals)\n",
    "    \"\"\"\n",
    "\n",
    "    # e = [abs(y_h - y_t[0]) for y_h, y_t in zip(y_hat, y_test)]\n",
    "    e = [abs(y_h - y_t) for y_h, y_t in zip(y_hat, y_test)]\n",
    "\n",
    "    if not smoothed:\n",
    "        return e\n",
    "\n",
    "    smoothing_window = int(batch_size * window_size * smoothing_perc)\n",
    "    if not len(y_hat) == len(y_test):\n",
    "        raise ValueError(\n",
    "            \"len(y_hat) != len(y_test), can't calculate error: %s (y_hat) , %s (y_test)\" % (len(y_hat), len(y_test)))\n",
    "\n",
    "    e_s = list(pd.DataFrame(e).ewm(span=smoothing_window).mean().values.flatten())\n",
    "\n",
    "    # for values at beginning < sequence length, just use avg\n",
    "    if anom is None:\n",
    "        e_s[:l_s] = [np.mean(e_s[:l_s * 2])] * l_s\n",
    "    elif not anom['chan_id'] == 'C-2':  # anom occurs early in window (limited data available for channel)\n",
    "        e_s[:l_s] = [np.mean(e_s[:l_s * 2])] * l_s\n",
    "\n",
    "    # np.save(os.path.join(\"data\", anom['run_id'], \"smoothed_errors\", anom[\"chan_id\"] + \".npy\"), np.array(e_s))\n",
    "\n",
    "    return e_s\n",
    "\n",
    "\n",
    "def process_errors(y_test, e_s, window_size = 30, batch_size=70, p=0.25):\n",
    "    '''Using windows of historical errors (h = batch size * window size), calculate the anomaly\n",
    "    threshold (epsilon) and group any anomalous error values into continuos sequences. Calculate\n",
    "    score for each sequence using the max distance from epsilon.\n",
    "\n",
    "    Args:\n",
    "        y_test (np array): test targets corresponding to true telemetry values at each timestep t\n",
    "        e_s (list): smoothed errors (residuals) between y_test and y_hat\n",
    "        \n",
    "    Optional:\n",
    "        Window_size: Sets the window size\n",
    "\n",
    "    Returns:\n",
    "        E_seq (list of tuples): Start and end indices for each anomaloues sequence\n",
    "        anom_scores (list): Score for each anomalous sequence\n",
    "    '''\n",
    "\n",
    "    i_anom = []  # anomaly indices\n",
    "\n",
    "    num_windows = int((y_test.shape[0] - (batch_size * window_size)) / batch_size)\n",
    "\n",
    "    # decrease the historical error window size (h) if number of test values is limited\n",
    "    while num_windows < 0:\n",
    "        window_size -= 1\n",
    "        if window_size <= 0:\n",
    "            window_size = 1\n",
    "        num_windows = int((y_test.shape[0] - (batch_size * window_size)) / batch_size)\n",
    "        if window_size == 1 and num_windows < 0:\n",
    "            raise ValueError(\"Batch_size (%s) larger than y_test (len=%s). Adjust batch_size.\" % (\n",
    "            batch_size, y_test.shape[0]))\n",
    "\n",
    "    # Identify anomalies for each new batch of values\n",
    "    for i in range(1, num_windows + 2):\n",
    "        prior_idx = (i - 1) * (batch_size)\n",
    "        idx = (window_size * batch_size) + ((i - 1) * batch_size)\n",
    "\n",
    "        if i == num_windows + 1:\n",
    "            idx = y_test.shape[0]\n",
    "\n",
    "        window_e_s = e_s[prior_idx:idx]\n",
    "        window_y_test = y_test[prior_idx:idx]\n",
    "\n",
    "        epsilon = find_epsilon(window_e_s, error_buffer)\n",
    "        window_anom_indices = get_anomalies(window_e_s, window_y_test, epsilon, i - 1, i_anom, len(y_test),p)\n",
    "\n",
    "        # update indices to reflect true indices in full set of values (not just window)\n",
    "        i_anom.extend([i_a + (i - 1) * batch_size for i_a in window_anom_indices])\n",
    "\n",
    "    # group anomalous indices into continuous sequences\n",
    "    i_anom = sorted(list(set(i_anom)))\n",
    "    groups = [list(group) for group in mit.consecutive_groups(i_anom)]\n",
    "    E_seq = [(g[0], g[-1]) for g in groups if not g[0] == g[-1]]\n",
    "\n",
    "    # calc anomaly scores based on max distance from epsilon for each sequence\n",
    "    anom_scores = []\n",
    "    for e_seq in E_seq:\n",
    "        score = max([abs(e_s[x] - epsilon) / (np.mean(e_s) + np.std(e_s)) for x in range(e_seq[0], e_seq[1])])\n",
    "        anom_scores.append(score)\n",
    "\n",
    "    return E_seq, anom_scores\n",
    "\n",
    "\n",
    "def find_epsilon(e_s, error_buffer, sd_lim=12.0):\n",
    "    '''Find the anomaly threshold that maximizes function representing tradeoff between a) number of anomalies\n",
    "    and anomalous ranges and b) the reduction in mean and st dev if anomalous points are removed from errors\n",
    "    (see https://arxiv.org/pdf/1802.04431.pdf)\n",
    "\n",
    "    Args:\n",
    "        e_s (array): residuals between y_test and y_hat values (smoothes using ewma)\n",
    "        error_buffer (int): if an anomaly is detected at a point, this is the number of surrounding values\n",
    "            to add the anomalous range. this promotes grouping of nearby sequences and more intuitive results\n",
    "        sd_lim (float): The max number of standard deviations above the mean to calculate as part of the\n",
    "            argmax function\n",
    "\n",
    "    Returns:\n",
    "        sd_threshold (float): the calculated anomaly threshold in number of standard deviations above the mean\n",
    "    '''\n",
    "\n",
    "    mean = np.mean(e_s)\n",
    "    sd = np.std(e_s)\n",
    "\n",
    "    max_s = 0\n",
    "    sd_threshold = sd_lim  # default if no winner or too many anomalous ranges\n",
    "\n",
    "    # it is possible for sd to be 0; avoid divide by zero error\n",
    "    if sd == 0:\n",
    "        return sd_threshold\n",
    "\n",
    "    for z in np.arange(2.5, sd_lim, 0.5):\n",
    "        epsilon = mean + (sd * z)\n",
    "        pruned_e_s, pruned_i, i_anom = [], [], []\n",
    "\n",
    "        for i, e in enumerate(e_s):\n",
    "            if e < epsilon:\n",
    "                pruned_e_s.append(e)\n",
    "                pruned_i.append(i)\n",
    "            if e > epsilon:\n",
    "                for j in range(0, error_buffer):\n",
    "                    if not i + j in i_anom and not i + j >= len(e_s):\n",
    "                        i_anom.append(i + j)\n",
    "                    if not i - j in i_anom and not i - j < 0:\n",
    "                        i_anom.append(i - j)\n",
    "\n",
    "        if len(i_anom) > 0:\n",
    "            # preliminarily group anomalous indices into continuous sequences (# sequences needed for scoring)\n",
    "            i_anom = sorted(list(set(i_anom)))\n",
    "            groups = [list(group) for group in mit.consecutive_groups(i_anom)]\n",
    "            E_seq = [(g[0], g[-1]) for g in groups if not g[0] == g[-1]]\n",
    "\n",
    "            perc_removed = 1.0 - (float(len(pruned_e_s)) / float(len(e_s)))\n",
    "            mean_perc_decrease = (mean - np.mean(pruned_e_s)) / mean\n",
    "            sd_perc_decrease = (sd - np.std(pruned_e_s)) / sd\n",
    "            s = (mean_perc_decrease + sd_perc_decrease) / (len(E_seq) ** 2 + len(i_anom))\n",
    "\n",
    "            # sanity checks\n",
    "            if s >= max_s and len(E_seq) <= 5 and len(i_anom) < (len(e_s) * 0.5):\n",
    "                sd_threshold = z\n",
    "                max_s = s\n",
    "\n",
    "    return sd_threshold  # multiply by sd to get epsilon\n",
    "\n",
    "\n",
    "def compare_to_epsilon(e_s, epsilon, len_y_test, inter_range, chan_std,\n",
    "                       std, error_buffer, window, i_anom_full):\n",
    "    '''Compare smoothed error values to epsilon (error threshold) and group consecutive errors together into\n",
    "    sequences.\n",
    "\n",
    "    Args:\n",
    "        e_s (list): smoothed errors between y_test and y_hat values\n",
    "        epsilon (float): Threshold for errors above which an error is considered anomalous\n",
    "        len_y_test (int): number of timesteps t in test data\n",
    "        inter_range (tuple of floats): range between 5th and 95 percentile values of error values\n",
    "        chan_std (float): standard deviation on test values\n",
    "        std (float): standard deviation of smoothed errors\n",
    "        error_buffer (int): number of values surrounding anomalous errors to be included in anomalous sequence\n",
    "        window (int): Count of number of error windows that have been processed\n",
    "        i_anom_full (list): list of all previously identified anomalies in test set\n",
    "\n",
    "    Returns:\n",
    "        E_seq (list of tuples): contains start and end indices of anomalous ranges\n",
    "        i_anom (list): indices of errors that are part of an anomlous sequnce\n",
    "        non_anom_max (float): highest smoothed error value below epsilon\n",
    "    '''\n",
    "\n",
    "    i_anom = []\n",
    "    E_seq = []\n",
    "    non_anom_max = 0\n",
    "\n",
    "    # Don't consider anything in window because scale of errors too small compared to scale of values\n",
    "    if not (std > (.05 * chan_std) or max(e_s) > (.05 * inter_range)) or not max(e_s) > 0.05:\n",
    "        return E_seq, i_anom, non_anom_max\n",
    "\n",
    "    # ignore initial error values until enough history for smoothing, prediction, comparisons\n",
    "    num_to_ignore = l_s * 2\n",
    "    # if y_test is small, ignore fewer\n",
    "    if len_y_test < 2500:\n",
    "        num_to_ignore = l_s\n",
    "    if len_y_test < 1800:\n",
    "        num_to_ignore = 0\n",
    "\n",
    "    for x in range(0, len(e_s)):\n",
    "\n",
    "        anom = True\n",
    "        if not e_s[x] > epsilon or not e_s[x] > 0.05 * inter_range:\n",
    "            anom = False\n",
    "\n",
    "        if anom:\n",
    "            for b in range(0, error_buffer):\n",
    "                if not x + b in i_anom and not x + b >= len(e_s) and (\n",
    "                        (x + b) >= len(e_s) - batch_size or window == 0):\n",
    "                    if not (window == 0 and x + b < num_to_ignore):\n",
    "                        i_anom.append(x + b)\n",
    "                # only considering new batch of values added to window, not full window\n",
    "                if not x - b in i_anom and ((x - b) >= len(e_s) - batch_size or window == 0):\n",
    "                    if not (window == 0 and x - b < num_to_ignore):\n",
    "                        i_anom.append(x - b)\n",
    "\n",
    "    # capture max of values below the threshold that weren't previously identified as anomalies\n",
    "    # (used in filtering process)\n",
    "    for x in range(0, len(e_s)):\n",
    "        adjusted_x = x + window * batch_size\n",
    "        if e_s[x] > non_anom_max and not adjusted_x in i_anom_full and not x in i_anom:\n",
    "            non_anom_max = e_s[x]\n",
    "\n",
    "    # group anomalous indices into continuous sequences\n",
    "    i_anom = sorted(list(set(i_anom)))\n",
    "    groups = [list(group) for group in mit.consecutive_groups(i_anom)]\n",
    "    E_seq = [(g[0], g[-1]) for g in groups if not g[0] == g[-1]]\n",
    "\n",
    "    return E_seq, i_anom, non_anom_max\n",
    "\n",
    "\n",
    "def prune_anoms(E_seq, e_s, non_anom_max, i_anom, p=0.25):\n",
    "    '''Remove anomalies that don't meet minimum separation from the next closest anomaly or error value\n",
    "\n",
    "    Args:\n",
    "        E_seq (list of lists): contains start and end indices of anomalous ranges\n",
    "        e_s (list): smoothed errors between y_test and y_hat values\n",
    "        non_anom_max (float): highest smoothed error value below epsilon\n",
    "        i_anom (list): indices of errors that are part of an anomlous sequnce\n",
    "        p (float): minimum percent decrease\n",
    "    Returns:\n",
    "        i_pruned (list): remaining indices of errors that are part of an anomlous sequnces\n",
    "            after pruning procedure\n",
    "    '''\n",
    "\n",
    "    E_seq_max, e_s_max = [], []\n",
    "    for e_seq in E_seq:\n",
    "        if len(e_s[e_seq[0]:e_seq[1]]) > 0:\n",
    "            E_seq_max.append(max(e_s[e_seq[0]:e_seq[1]]))\n",
    "            e_s_max.append(max(e_s[e_seq[0]:e_seq[1]]))\n",
    "    e_s_max.sort(reverse=True)\n",
    "\n",
    "    if non_anom_max and non_anom_max > 0:\n",
    "        e_s_max.append(non_anom_max)  # for comparing the last actual anomaly to next highest below epsilon\n",
    "\n",
    "    i_to_remove = []\n",
    "    #p = 0.25  # TODO: don't hardcode this\n",
    "\n",
    "    for i in range(0, len(e_s_max)):\n",
    "        if i + 1 < len(e_s_max):\n",
    "            if (e_s_max[i] - e_s_max[i + 1]) / e_s_max[i] < p:\n",
    "                i_to_remove.append(E_seq_max.index(e_s_max[i]))\n",
    "                # p += 0.03 # increase minimum separation by this amount for each step further from max error\n",
    "            else:\n",
    "                i_to_remove = []\n",
    "    for idx in sorted(i_to_remove, reverse=True):\n",
    "        del E_seq[idx]\n",
    "\n",
    "    i_pruned = []\n",
    "    for i in i_anom:\n",
    "        keep_anomaly_idx = False\n",
    "\n",
    "        for e_seq in E_seq:\n",
    "            if i >= e_seq[0] and i <= e_seq[1]:\n",
    "                keep_anomaly_idx = True\n",
    "\n",
    "        if keep_anomaly_idx == True:\n",
    "            i_pruned.append(i)\n",
    "\n",
    "    return i_pruned\n",
    "\n",
    "\n",
    "def get_anomalies(e_s, y_test, z, window, i_anom_full, len_y_test, p=0.25):\n",
    "    '''Find anomalous sequences of smoothed error values that are above error threshold (epsilon). Both\n",
    "    smoothed errors and the inverse of the smoothed errors are evaluated - large dips in errors often\n",
    "    also indicate anomlies.\n",
    "\n",
    "    Args:\n",
    "        e_s (list): smoothed errors between y_test and y_hat values\n",
    "        y_test (np array): test targets corresponding to true telemetry values at each timestep for given window\n",
    "        z (float): number of standard deviations above mean corresponding to epsilon\n",
    "        window (int): number of error windows that have been evaluated\n",
    "        i_anom_full (list): list of all previously identified anomalies in test set\n",
    "        len_y_test (int): num total test values available in dataset\n",
    "\n",
    "    Returns:\n",
    "        i_anom (list): indices of errors that are part of an anomlous sequnces\n",
    "    '''\n",
    "\n",
    "    perc_high, perc_low = np.percentile(y_test, [95, 5])\n",
    "    inter_range = perc_high - perc_low\n",
    "\n",
    "    mean = np.mean(e_s)\n",
    "    std = np.std(e_s)\n",
    "    chan_std = np.std(y_test)\n",
    "\n",
    "    e_s_inv = [mean + (mean - e) for e in e_s]  # flip it around the mean\n",
    "    z_inv = find_epsilon(e_s_inv, error_buffer)\n",
    "\n",
    "    epsilon = mean + (float(z) * std)\n",
    "    epsilon_inv = mean + (float(z_inv) * std)\n",
    "\n",
    "    # find sequences of anomalies greater than epsilon\n",
    "    E_seq, i_anom, non_anom_max = compare_to_epsilon(e_s, epsilon, len_y_test,\n",
    "                                                     inter_range, chan_std, std, error_buffer, window,\n",
    "                                                     i_anom_full)\n",
    "\n",
    "    # find sequences of anomalies using inverted error values (lower than normal errors are also anomalous)\n",
    "    E_seq_inv, i_anom_inv, inv_non_anom_max = compare_to_epsilon(e_s_inv, epsilon_inv,\n",
    "                                                                 len_y_test, inter_range, chan_std, std,\n",
    "                                                                 error_buffer, window, i_anom_full)\n",
    "\n",
    "    if len(E_seq) > 0:\n",
    "        i_anom = prune_anoms(E_seq, e_s, non_anom_max, i_anom, p)\n",
    "\n",
    "    if len(E_seq_inv) > 0:\n",
    "        i_anom_inv = prune_anoms(E_seq_inv, e_s_inv, inv_non_anom_max, i_anom_inv, p)\n",
    "\n",
    "    i_anom = list(set(i_anom + i_anom_inv))\n",
    "\n",
    "    return i_anom\n",
    "\n",
    "\n",
    "# Not using because I don't have labeled anomalies\n",
    "# def evaluate_sequences(E_seq, anom):\n",
    "#     '''Compare identified anomalous sequences with labeled anomalous sequences\n",
    "#\n",
    "#     Args:\n",
    "#         E_seq (list of lists): contains start and end indices of anomalous ranges\n",
    "#         anom (dict): contains anomaly information for a given input stream\n",
    "#\n",
    "#     Returns:\n",
    "#         anom (dict): with updated anomaly information (whether identified, scores, etc.)\n",
    "#     '''\n",
    "#\n",
    "#     anom[\"false_positives\"] = 0\n",
    "#     anom[\"false_negatives\"] = 0\n",
    "#     anom[\"true_positives\"] = 0\n",
    "#     anom[\"fp_sequences\"] = []\n",
    "#     anom[\"tp_sequences\"] = []\n",
    "#     anom[\"num_anoms\"] = len(anom[\"anomaly_sequences\"])\n",
    "#\n",
    "#     E_seq_test = eval(anom[\"anomaly_sequences\"])\n",
    "#\n",
    "#     if len(E_seq) > 0:\n",
    "#\n",
    "#         matched_E_seq_test = []\n",
    "#\n",
    "#         for e_seq in E_seq:\n",
    "#\n",
    "#             valid = False\n",
    "#\n",
    "#             for i, a in enumerate(E_seq_test):\n",
    "#\n",
    "#                 if (e_seq[0] >= a[0] and e_seq[0] <= a[1]) or (e_seq[1] >= a[0] and e_seq[1] <= a[1]) or \\\n",
    "#                         (e_seq[0] <= a[0] and e_seq[1] >= a[1]) or (a[0] <= e_seq[0] and a[1] >= e_seq[1]):\n",
    "#\n",
    "#                     anom[\"tp_sequences\"].append(e_seq)\n",
    "#\n",
    "#                     valid = True\n",
    "#\n",
    "#                     if i not in matched_E_seq_test:\n",
    "#                         anom[\"true_positives\"] += 1\n",
    "#                         matched_E_seq_test.append(i)\n",
    "#\n",
    "#             if valid == False:\n",
    "#                 anom[\"false_positives\"] += 1\n",
    "#                 anom[\"fp_sequences\"].append([e_seq[0], e_seq[1]])\n",
    "#\n",
    "#         anom[\"false_negatives\"] += (len(E_seq_test) - len(matched_E_seq_test))\n",
    "#\n",
    "#     else:\n",
    "#         anom[\"false_negatives\"] += len(E_seq_test)\n",
    "#\n",
    "#     return anom\n",
    "\n",
    "#####################################\n",
    "# Function was created by CO Boulder student, Shawn Polson\n",
    "# Adjusted for this project by improving outputs and better choices of parameters by eliminating hardcodes of variables\n",
    "\n",
    "def detect_anomalies(ts, normal_model, ds_name, var_name, alg_name, window_size = 30, batch_size=70, smoothing_perc=0.05,\n",
    "                     p=0.25,outlier_def='dynamic', num_stds=2, ndt_errors=None,\n",
    "                     plot_save_path=None, data_save_path=None):\n",
    "    \"\"\"Detect outliers in the time series data by comparing points against a \"normal\" model.\n",
    "       Inputs:\n",
    "           ts [pd Series]:           A pandas Series with a DatetimeIndex and a column for numerical values.\n",
    "           normal_model [pd Series]: A pandas Series with a DatetimeIndex and a column for numerical values.\n",
    "           ds_name [str]:            The name of the time series dataset.\n",
    "           var_name [str]:           The name of the dependent variable in the time series.\n",
    "           alg_name [str]:           The name of the algorithm used to create 'normal_model'.\n",
    "       Optional Inputs:\n",
    "           outlier_def [str]:    {'std', 'errors', 'dynamic'} The definition of an outlier to be used. Can be 'std' for [num_stds] from the data's mean,\n",
    "                                 'errors' for [num_stds] from the mean of the errors, or 'dynamic' for nonparametric dynamic thresholding\n",
    "                                 Default is 'std'.\n",
    "           num_stds [float]:     The number of standard deviations away from the mean used to define point outliers (when applicable).\n",
    "                                 Default is 2.\n",
    "           ndt_errors [list]:    Optionally skip nonparametric dynamic thresholding's 'get_errors()' and use these values instead.\n",
    "           plot_save_path [str]: The file path (ending in file name *.png) for saving plots of outliers.\n",
    "           data_save_path [str]: The file path (ending in file name *.csv) for saving CSVs with outliers.\n",
    "       Outputs:\n",
    "           time_series_with_outliers [pd DataFrame]: A pandas DataFrame with a DatetimeIndex, two columns for numerical values, and an Outlier column (True or False).\n",
    "       Optional Outputs:\n",
    "           None\n",
    "       Example:\n",
    "           time_series_with_outliers = detect_anomalies(time_series, model, 'BatteryTemperature', 'Temperature (C)',\n",
    "                                                        'ARIMA', 'dynamic', plot_path, data_path)\n",
    "    \"\"\"\n",
    "\n",
    "    X = ts.values\n",
    "    Y = normal_model.values\n",
    "    outliers = pd.Series()\n",
    "    errors = pd.Series()\n",
    "    time_series_with_outliers = pd.DataFrame({var_name: ts, alg_name: normal_model})\n",
    "    time_series_with_outliers['Outlier'] = 'False'\n",
    "    column_names = [var_name, alg_name, 'Outlier']  # column order\n",
    "    time_series_with_outliers = time_series_with_outliers.reindex(columns=column_names)  # sort columns in specified order\n",
    "\n",
    "    # Start a progress bar\n",
    "    widgets = [progressbar.Percentage(), progressbar.Bar(), progressbar.Timer(), ' ', progressbar.AdaptiveETA()]\n",
    "    progress_bar_sliding_window = progressbar.ProgressBar(\n",
    "        widgets=[progressbar.FormatLabel('Outliers (' + ds_name + ')')] + widgets,\n",
    "        maxval=int(len(X))).start()\n",
    "\n",
    "\n",
    "    # Define outliers using JPL's nonparamatric dynamic thresholding technique\n",
    "    if outlier_def == 'dynamic':\n",
    "        progress_bar_sliding_window.update(int(len(X))/2)  # start progress bar timer\n",
    "        outlier_points = []\n",
    "        outlier_indices = []\n",
    "        if ndt_errors is not None:\n",
    "            smoothed_errors = ndt_errors\n",
    "        else:\n",
    "            smoothed_errors = get_errors(X, Y,window_size, batch_size, smoothing_perc)\n",
    "            time_series_with_outliers['errors'] = smoothed_errors\n",
    "            \n",
    "        # These are the results of the nonparametric dynamic thresholding\n",
    "        E_seq, anom_scores = process_errors(X, smoothed_errors,window_size, batch_size, p)\n",
    "        progress_bar_sliding_window.update(int(len(X)) - 1)  # advance progress bar timer\n",
    "\n",
    "        # Convert sets of outlier start/end indices into outlier points\n",
    "        for anom in E_seq:\n",
    "            start = anom[0]\n",
    "            end = anom[1]\n",
    "            for i in range(start, end+1):\n",
    "                time_series_with_outliers.at[ts.index[i], 'Outlier'] = 'True'\n",
    "                outlier_points.append(X[i])\n",
    "                outlier_indices.append(ts.index[i])\n",
    "        outliers = outliers.append(pd.Series(outlier_points, index=outlier_indices))\n",
    "        \n",
    "    # Plot anomalies\n",
    "    ax = ts.plot(color='#192C87', title=ds_name + ' with ' + alg_name + ' Outliers', label=var_name, figsize=(14, 6))\n",
    "    normal_model.plot(color='#0CCADC', label=alg_name, linewidth=1.5)\n",
    "    if len(outliers) > 0:\n",
    "        print('Detected outliers (' + ds_name + '): ' + str(len(outliers)))\n",
    "        outliers.plot(color='red', style='.', label='Outliers')\n",
    "    ax.set(xlabel='Time', ylabel=var_name)\n",
    "    pyplot.legend(loc='best')\n",
    "\n",
    "    # Save plot\n",
    "    if plot_save_path is not None:\n",
    "        plot_dir = plot_save_path[:plot_save_path.rfind('/')+1]\n",
    "        if not os.path.exists(plot_dir):\n",
    "            os.makedirs(plot_dir)\n",
    "        pyplot.savefig(plot_save_path, dpi=500)\n",
    "\n",
    "    pyplot.show()\n",
    "    pyplot.clf()\n",
    "\n",
    "    # Save data\n",
    "    if data_save_path is not None:\n",
    "        data_dir = data_save_path[:data_save_path.rfind('/')+1]\n",
    "        if not os.path.exists(data_dir):\n",
    "            os.makedirs(data_dir)\n",
    "        time_series_with_outliers.to_csv(data_save_path)\n",
    "\n",
    "    return time_series_with_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fa9813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
